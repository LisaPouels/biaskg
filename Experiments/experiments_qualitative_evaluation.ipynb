{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b0adae",
   "metadata": {},
   "source": [
    "# Qualitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329aef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_name(filename):\n",
    "    \"\"\"\n",
    "    Extracts the model name from the filename.\n",
    "    \"\"\"\n",
    "    return filename.split('_')[0]\n",
    "\n",
    "def extract_k_value(filename):\n",
    "    \"\"\"\n",
    "    Extracts the k value from the filename.\n",
    "    \"\"\"\n",
    "    return int(os.path.basename(filename).split('_')[1].split('k')[1])\n",
    "\n",
    "def extract_retriever_method(filename):\n",
    "    \"\"\"\n",
    "    Extracts the retriever method from the filename.\n",
    "    \"\"\"\n",
    "    retriever_name = os.path.basename(filename).split('_')[2]\n",
    "    retriever_type = os.path.basename(filename).split('_')[3]\n",
    "    if retriever_name == \"Original\" and retriever_type == \"Original\":\n",
    "        return \"Original\"\n",
    "    elif retriever_name == \"Original\" and retriever_type == \"Pagerank\":\n",
    "        return \"Pruning\"\n",
    "    elif retriever_name == \"Reranker\" and retriever_type == \"Original\":\n",
    "        return \"Reranking\"\n",
    "    elif retriever_name == \"Reranker\" and retriever_type == \"Pagerank\":\n",
    "        return \"Reranking+Pruning\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown retriever method: {retriever_name} {retriever_type}\")\n",
    "\n",
    "#TODO: check if the FN and FP are correct - if yes, change labels in the plots below?\n",
    "def classify_outcome(row):\n",
    "    if row['rag_answer_correct'] and row['target'] == 'Target':\n",
    "        return 'TP'\n",
    "    elif not row['rag_answer_correct'] and row['target'] == 'Target':\n",
    "        return 'FP'\n",
    "    elif not row['rag_answer_correct'] and row['target'] == 'Non-Target':\n",
    "        return 'FN'\n",
    "    elif row['rag_answer_correct'] and row['target'] == 'Non-Target':\n",
    "        return 'TN'\n",
    "    elif row['rag_answer_correct'] and row['target'] == 'Unknown':\n",
    "        return 'TN'\n",
    "    elif not row['rag_answer_correct'] and row['target'] == 'Unknown':\n",
    "        return 'FN'\n",
    "    else:\n",
    "        return 'Unclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c3d55",
   "metadata": {},
   "source": [
    "## Experiment 1 (LLM choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70a2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2fb3500",
   "metadata": {},
   "source": [
    "## Experiment 2 (Retriever)\n",
    "\n",
    "### 2a: K-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0752bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134 CSV files in 2_Retriever/2a_k_value\n"
     ]
    }
   ],
   "source": [
    "csv_folder = \"2_Retriever/2a_k_value\"\n",
    "csv_files = glob.glob(os.path.join(csv_folder, \"*.csv\"))\n",
    "print(f\"Found {len(csv_files)} CSV files in {csv_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2471176",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for file_path in csv_files:\n",
    "    # Extract the model name from the filename\n",
    "    model_name = extract_model_name(os.path.basename(file_path))\n",
    "    # Extract the k value from the filename\n",
    "    k_value = extract_k_value(os.path.basename(file_path))\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    all_dfs[model_name][k_value].append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ba56a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_prompt_answers = defaultdict(dict)\n",
    "\n",
    "for model, k_values in all_dfs.items():\n",
    "    for k, dfs in k_values.items():\n",
    "        answers = pd.DataFrame([df['rag_answer_correct'] for df in dfs])\n",
    "        mode_answers = answers.mode(axis=0).iloc[0]\n",
    "        config_prompt_answers[model][k] = mode_answers\n",
    "\n",
    "# print(config_prompt_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffea1dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed prompt indices per model:\n",
      "falcon: [9, 15, 23, 24, 30, 38, 57, 61, 66, 69, 72, 78, 86, 87, 113]\n",
      "gpt-4.1-nano: [13, 15, 18, 26, 28, 30, 33, 34, 37, 45, 46, 55, 64, 68, 70, 72, 75, 78, 80, 81, 83, 86, 89, 97, 104, 105, 108, 112, 114, 116]\n",
      "qwen2.5: [0, 3, 9, 16, 22, 26, 32, 41, 44, 45, 50, 58, 67, 71, 80, 86, 89, 90, 97, 98, 100, 101, 104, 108, 110, 112, 116, 120]\n",
      "deepseek-v2: [6, 7, 9, 15, 17, 22, 23, 34, 36, 38, 41, 43, 56, 58, 60, 65, 68, 76, 81, 85, 88, 89, 92, 97, 101, 104, 105, 114, 124]\n",
      "mistral: [5, 9, 10, 12, 14, 17, 20, 22, 27, 30, 31, 34, 41, 47, 52, 58, 59, 60, 63, 64, 67, 70, 73, 74, 75, 80, 83, 84, 91, 93, 95, 98, 106, 110, 114, 116, 118, 124]\n",
      "gemini-2.0-flash: []\n",
      "llama3.2: [1, 4, 8, 10, 12, 14, 20, 21, 24, 26, 28, 33, 35, 36, 37, 41, 46, 48, 53, 56, 65, 75, 84, 85, 88, 89, 90, 92, 95, 101, 105, 110, 111, 117, 118, 123]\n"
     ]
    }
   ],
   "source": [
    "changed_prompt_indices_per_model = {}\n",
    "\n",
    "for model, k_answers in config_prompt_answers.items():\n",
    "    answer_df = pd.DataFrame(k_answers)  # rows = prompts, columns = k-values\n",
    "    # Find rows where answers differ across k-values\n",
    "    changed = answer_df.nunique(axis=1) > 1\n",
    "    changed_prompt_indices_per_model[model] = answer_df[changed].index.tolist()\n",
    "\n",
    "print(\"Changed prompt indices per model:\")\n",
    "for model, indices in changed_prompt_indices_per_model.items():\n",
    "    print(f\"{model}: {indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff1ebe3",
   "metadata": {},
   "source": [
    "### 2b: Retriever method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "433172de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 122 CSV files in 2_Retriever/2b_Retriever\n"
     ]
    }
   ],
   "source": [
    "csv_folder = \"2_Retriever/2b_Retriever\"\n",
    "csv_files = glob.glob(os.path.join(csv_folder, \"*.csv\"))\n",
    "print(f\"Found {len(csv_files)} CSV files in {csv_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9176a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for file_path in csv_files:\n",
    "    # Extract the model name from the filename\n",
    "    model_name = extract_model_name(os.path.basename(file_path))\n",
    "    # Extract the k value from the filename\n",
    "    k_value = extract_k_value(os.path.basename(file_path))\n",
    "    # Extract the retriever method from the filename\n",
    "    retriever_method = extract_retriever_method(os.path.basename(file_path))\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    all_dfs[model_name][retriever_method].append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16e56b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_prompt_answers = defaultdict(dict)\n",
    "\n",
    "for model, retrievers in all_dfs.items():\n",
    "    for r, dfs in retrievers.items():\n",
    "        answers = pd.DataFrame([df['rag_answer_correct'] for df in dfs])\n",
    "        mode_answers = answers.mode(axis=0).iloc[0]\n",
    "        config_prompt_answers[model][r] = mode_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a55a2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed prompt indices per model:\n",
      "deepseek-v2: [6, 8, 9, 10, 14, 24, 26, 29, 32, 34, 41, 49, 56, 66, 67, 68, 72, 76, 78, 80, 83, 85, 87, 92, 97, 102, 104, 111, 124]\n",
      "llama3.2: [1, 6, 13, 20, 21, 28, 48, 51, 53, 54, 60, 72, 75, 77, 101, 109, 117]\n",
      "gpt-4.1-nano: [7, 8, 11, 15, 16, 17, 30, 33, 34, 45, 51, 55, 62, 64, 72, 78, 80, 83, 85, 86, 97, 103, 104, 112, 116]\n",
      "falcon: [2, 7, 17, 18, 19, 23, 24, 61, 78]\n",
      "qwen2.5: [9, 11, 15, 16, 22, 23, 25, 41, 49, 56, 58, 62, 66, 76, 97, 100, 109, 110, 112, 113, 122]\n",
      "mistral: [7, 9, 12, 14, 17, 29, 30, 52, 60, 61, 66, 67, 68, 73, 80, 82, 83, 91, 93, 94, 97, 99, 106, 116, 117, 118, 123, 124]\n"
     ]
    }
   ],
   "source": [
    "changed_prompt_indices_per_model = {}\n",
    "\n",
    "for model, retrievers in config_prompt_answers.items():\n",
    "    answer_df = pd.DataFrame(retrievers)  # rows = prompts, columns = k-values\n",
    "    # Find rows where answers differ across k-values\n",
    "    changed = answer_df.nunique(axis=1) > 1\n",
    "    changed_prompt_indices_per_model[model] = answer_df[changed].index.tolist()\n",
    "\n",
    "print(\"Changed prompt indices per model:\")\n",
    "for model, indices in changed_prompt_indices_per_model.items():\n",
    "    print(f\"{model}: {indices}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
