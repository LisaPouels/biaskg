{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d673862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from flashrank import Ranker, RerankRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e53ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nano (~4MB), blazing fast model & competitive performance (ranking precision).\n",
    "\n",
    "ranker = Ranker(max_length=128)\n",
    "\n",
    "# or \n",
    "\n",
    "# # Small (~34MB), slightly slower & best performance (ranking precision).\n",
    "# ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"/opt\")\n",
    "\n",
    "# or \n",
    "\n",
    "# # Medium (~110MB), slower model with best zeroshot performance (ranking precision) on out of domain data.\n",
    "# ranker = Ranker(model_name=\"rank-T5-flan\", cache_dir=\"/opt\")\n",
    "\n",
    "# or \n",
    "\n",
    "# # Medium (~150MB), slower model with competitive performance (ranking precision) for 100+ languages  (don't use for english)\n",
    "# ranker = Ranker(model_name=\"ms-marco-MultiBERT-L-12\", cache_dir=\"/opt\")\n",
    "\n",
    "# or \n",
    "\n",
    "# ranker = Ranker(model_name=\"rank_zephyr_7b_v1_full\", max_length=1024) # adjust max_length based on your passage length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0827974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 4, 'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.', 'meta': {'additional': 'info4'}, 'score': 0.017240638}, {'id': 5, 'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels', 'meta': {'additional': 'info5'}, 'score': 0.011028998}, {'id': 1, 'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.', 'meta': {'additional': 'info1'}, 'score': 0.0006223073}, {'id': 2, 'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper', 'meta': {'additional': 'info2'}, 'score': 0.0002462871}, {'id': 3, 'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\", 'meta': {'additional': 'info3'}, 'score': 8.866134e-05}]\n"
     ]
    }
   ],
   "source": [
    "# Metadata is optional, Id can be your DB ids from your retrieval stage or simple numeric indices.\n",
    "query = \"How to speedup LLMs?\"\n",
    "passages = [\n",
    "   {\n",
    "      \"id\":1,\n",
    "      \"text\":\"Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.\",\n",
    "      \"meta\": {\"additional\": \"info1\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":2,\n",
    "      \"text\":\"LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper\",\n",
    "      \"meta\": {\"additional\": \"info2\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":3,\n",
    "      \"text\":\"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
    "      \"meta\": {\"additional\": \"info3\"}\n",
    "\n",
    "   },\n",
    "   {\n",
    "      \"id\":4,\n",
    "      \"text\":\"Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.\",\n",
    "      \"meta\": {\"additional\": \"info4\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":5,\n",
    "      \"text\":\"vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels\",\n",
    "      \"meta\": {\"additional\": \"info5\"}\n",
    "   }\n",
    "]\n",
    "\n",
    "rerankrequest = RerankRequest(query=query, passages=passages)\n",
    "results = ranker.rerank(rerankrequest)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdbdc91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
